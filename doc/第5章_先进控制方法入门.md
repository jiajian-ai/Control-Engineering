# 第5章 先进控制方法入门

> **课程大纲导航**: [返回课程大纲](../CONTROL_ENGINEERING_OUTLINE.md) | [上一章：现代控制理论](第4章_现代控制理论基础.md) | [下一章：典型工程案例](第6章_典型工程案例分析.md)

---

## 📚 本章概述

本章深入介绍经典PID控制和现代状态空间方法之外的**先进控制技术**，重点讲解**模型预测控制（MPC）**，并系统介绍优化控制、约束处理、自适应控制、鲁棒控制、前馈控制以及人工智能控制（强化学习）等方法。

**本章学习目标**：
1. 深入理解MPC的核心思想、数学原理和实现细节
2. 掌握优化控制的建模方法、约束处理和求解技术
3. 了解自适应、鲁棒、前馈控制的基本概念和应用场景
4. 认识强化学习在控制领域的作用及其与传统方法的关系
5. 学会根据系统特性选择合适的先进控制方法

**为什么需要先进控制方法？**

经典PID和基本状态反馈的局限：
- ❌ 难以处理**多变量强耦合**系统
- ❌ **约束**处理不优（简单截断）
- ❌ **非线性**系统性能受限
- ❌ **不确定性**鲁棒性不足
- ❌ 缺乏**预测能力**

先进控制方法的优势：
- ✅ 系统化的优化框架
- ✅ 显式约束处理
- ✅ 预测未来演化
- ✅ 自适应参数变化
- ✅ 鲁棒性保证

---

## 5.1 模型预测控制（MPC）

### 5.1.1 MPC的诞生与发展

#### 历史背景

**1970年代**：MPC诞生于石油化工行业
- 法国Adersa公司的**IDCOM**（1974）
- Shell石油的**DMC**（Dynamic Matrix Control, 1979）
- 解决多变量、大时滞过程控制问题

**1980年代**：理论发展
- 稳定性理论建立
- 终端约束与终端代价
- 学术界开始重视

**1990年代至今**：广泛应用
- 化工、炼油、电力、航空航天
- 汽车（自适应巡航、车道保持）
- 机器人（轨迹规划、避障）
- 建筑能耗管理

**为什么MPC如此成功？**
1. **约束处理能力**：工业过程普遍存在约束
2. **多变量优化**：统一处理耦合系统
3. **预测能力**：提前规划，减少超调
4. **直观调节**：权重矩阵调节性能指标

### 5.1.2 MPC的核心思想

**四大支柱**：

```
       ┌─────────────┐
       │   预测模型   │ ← 基于模型预测未来
       └──────┬──────┘
              ↓
       ┌─────────────┐
       │  滚动优化   │ ← 在线求解最优控制
       └──────┬──────┘
              ↓
       ┌─────────────┐
       │  反馈校正   │ ← 测量反馈纠正模型
       └──────┬──────┘
              ↓
       ┌─────────────┐
       │  执行首步   │ ← 仅执行第一步控制
       └─────────────┘
              ↓
        （下一时刻重复）
```

**关键概念**：

1. **预测时域（Prediction Horizon, N）**：
   - 预测未来N步的系统状态
   - 类比：开车时"看前方多远"

2. **控制时域（Control Horizon, M）**：
   - 优化前M步的控制输入（M ≤ N）
   - 类比：规划未来几步的操作

3. **滚动时域（Receding Horizon）**：
   - 每步只执行第一个控制
   - 下一步重新优化
   - 类比：走一步看一步（但每次都看很远）

**与其他方法的区别**：

| 方法 | 时域考虑 | 优化 | 约束 |
|------|---------|------|------|
| **PID** | 当前时刻 | 无显式优化 | 简单截断 |
| **LQR** | 无限时域 | 离线优化（固定K） | 难处理 |
| **MPC** | 有限时域 | 在线优化 | 显式处理 |

### 5.1.3 MPC的数学表达

#### 离散时间系统模型

**线性系统**：

$$x_{k+1} = Ax_k + Bu_k$$

$$y_k = Cx_k$$

**非线性系统**：

$$x_{k+1} = f(x_k, u_k)$$

$$y_k = h(x_k)$$

#### 标准优化问题

**目标函数**（二次型）：

$$\min_{u_0, u_1, \ldots, u_{M-1}} J = \sum_{i=1}^{N} \|x_i - x_{ref}\|_Q^2 + \sum_{i=0}^{M-1} \|u_i\|_R^2 + \|x_N - x_{ref}\|_P^2$$

其中：
- $\|z\|_Q^2 = z^T Q z$：加权范数
- $Q \succeq 0$：状态权重矩阵（半正定）
- $R \succ 0$：控制权重矩阵（正定）
- $P \succeq 0$：终端权重矩阵
- $N$：预测步数
- $M$：控制步数

**约束条件**：

$$\begin{aligned}
& x_{k+1} = Ax_k + Bu_k, \quad k = 0, \ldots, N-1 & \text{(系统动力学)} \\
& x_0 = x(t) & \text{(初始条件)} \\
& u_{min} \leq u_k \leq u_{max}, \quad k = 0, \ldots, M-1 & \text{(输入约束)} \\
& x_{min} \leq x_k \leq x_{max}, \quad k = 1, \ldots, N & \text{(状态约束)} \\
& y_{min} \leq y_k \leq y_{max}, \quad k = 1, \ldots, N & \text{(输出约束)}
\end{aligned}$$

**控制变化率约束**（可选）：

$$\Delta u_{min} \leq u_k - u_{k-1} \leq \Delta u_{max}$$

代价函数加入惩罚：

$$J = \cdots + \sum_{i=0}^{M-2} \|\Delta u_i\|_{R_\Delta}^2$$

#### MPC算法流程

**伪代码**：

```python
# 初始化
t = 0
x_current = x_initial

while t < simulation_time:
    # 1. 测量当前状态
    x_measured = measure_state()
    
    # 2. 构建优化问题
    #    变量：u_0, u_1, ..., u_{M-1}
    #    目标：min J(x, u)
    #    约束：动力学 + 边界条件
    
    # 3. 求解优化
    u_optimal_sequence = solve_optimization(x_measured)
    
    # 4. 执行第一个控制
    u_current = u_optimal_sequence[0]
    apply_control(u_current)
    
    # 5. 系统演化
    x_current = system_dynamics(x_current, u_current, dt)
    
    # 6. 时间前进
    t = t + dt
```

### 5.1.4 线性MPC详解

#### 二次规划（QP）形式

对于线性系统，MPC可转化为标准QP问题。

**状态空间堆叠**：

定义增广向量：

$$X = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}, \quad
U = \begin{bmatrix} u_0 \\ u_1 \\ \vdots \\ u_{M-1} \end{bmatrix}$$

**预测方程**：

$$x_1 = Ax_0 + Bu_0$$
$$x_2 = Ax_1 + Bu_1 = A^2x_0 + ABu_0 + Bu_1$$
$$\vdots$$

堆叠形式：

$$X = \mathcal{A}x_0 + \mathcal{B}U$$

其中：

$$\mathcal{A} = \begin{bmatrix} A \\ A^2 \\ \vdots \\ A^N \end{bmatrix}, \quad
\mathcal{B} = \begin{bmatrix} 
B & 0 & \cdots & 0 \\
AB & B & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
A^{N-1}B & A^{N-2}B & \cdots & A^{N-M}B
\end{bmatrix}$$

**QP标准形式**：

$$\min_U \frac{1}{2}U^T H U + f^T U$$

$$\text{subject to} \quad G U \leq h$$

其中：
- $H = 2(\mathcal{B}^T \bar{Q} \mathcal{B} + \bar{R})$
- $f = 2\mathcal{B}^T \bar{Q}(\mathcal{A}x_0 - X_{ref})$
- $G, h$：约束矩阵

**求解器**：
- OSQP（开源）
- qpOASES（实时）
- CPLEX（商业）
- Gurobi（商业）

#### 稳定性保证

**无约束MPC**：
- 等价于LQR（无限时域）
- 保证稳定性

**有约束MPC**：
需要额外条件保证稳定性

**方法1：终端约束**

$$x_N = 0 \text{ 或 } x_N \in \mathcal{X}_f$$

其中 $\mathcal{X}_f$ 是终端不变集。

**方法2：终端代价**

选择合适的终端权重 $P$（如LQR的Riccati解）：

$$J = \cdots + \|x_N\|_P^2$$

使得 $P$ 是Lyapunov函数。

**方法3：足够长的预测时域**

如果 $N$ 足够大，系统自然收敛。

### 5.1.5 非线性MPC详解

#### 非线性优化问题

**目标函数**：

$$\min_{u_0, \ldots, u_{M-1}} J = \sum_{i=1}^{N} L(x_i, u_i) + F(x_N)$$

其中：
- $L(x, u)$：阶段代价（stage cost）
- $F(x_N)$：终端代价（terminal cost）

**约束**：

$$\begin{aligned}
& x_{k+1} = f(x_k, u_k) \\
& g(x_k, u_k) \leq 0 \\
& h(x_k, u_k) = 0
\end{aligned}$$

#### 求解方法

**方法1：序列二次规划（SQP）**

迭代线性化：
1. 在当前点线性化非线性约束
2. 求解QP子问题
3. 更新迭代点
4. 重复直至收敛

**方法2：内点法（Interior Point）**

将不等式约束转化为barrier函数，求解无约束优化。

**方法3：多重打靶法（Multiple Shooting）**

将长时域优化分解为多段短时优化，提高数值稳定性。

#### 计算挑战

**实时性问题**：
- 非线性优化计算慢
- 不保证全局最优
- 可能不收敛

**解决方案**：

1. **实时迭代（RTI）**：
   - 每步只做一次SQP迭代
   - 预热（warmstart）下次优化

2. **显式MPC**：
   - 离线计算所有可能情况的最优解
   - 在线查表
   - 仅适用于小规模系统

3. **学习型MPC**：
   - 神经网络学习最优策略
   - 在线微调

### 5.1.6 MPC参数设计

#### 预测时域 N

**物理意义**：控制器"看多远"

**选择原则**：
- 覆盖系统主要动态（$N \cdot \Delta t \geq 3\tau$）
- 足够"看到"参考轨迹变化
- 受限于计算能力

**示例**：
- 温度系统（$\tau = 20$s，$\Delta t = 10$s）：$N = 10 \sim 20$
- CartPole（$\tau \approx 0.1$s，$\Delta t = 0.02$s）：$N = 10 \sim 15$

**影响**：
- $N$太小 → 短视，性能差
- $N$太大 → 计算慢，收益递减

#### 控制时域 M

**物理意义**：优化前M步控制，后面保持不变

**选择原则**：
- $M = N$：全自由度（计算量大）
- $M = N/2$：折中（常用）
- $M = 1$：仅优化当前步（极端）

**分块控制时域**：
- 前几步：每步独立优化
- 后几步：保持恒定
- 平衡性能与计算量

#### 权重矩阵 Q 和 R

**Q矩阵设计**（状态权重）：

$$Q = \text{diag}(q_1, q_2, \ldots, q_n)$$

**原则**：
- 重要状态 → 大权重
- 次要状态 → 小权重
- 不关心的状态 → 零权重

**归一化**：

$$q_i = \frac{w_i}{(x_i^{max} - x_i^{min})^2}$$

其中 $w_i$ 是相对重要性。

**示例（CartPole）**：

```python
Q = np.diag([10,    # 位置（希望在中心）
             1,     # 速度（不直接惩罚）
             200,   # 角度（最重要！）
             20])   # 角速度（阻尼）
```

**R矩阵设计**（控制权重）：

$$R = r \cdot I_m$$

**原则**：
- $r$ 大 → 控制"昂贵"，输出平滑但慢
- $r$ 小 → 控制"便宜"，响应快但可能抖动

**权衡**：
- 跟踪精度 vs 控制代价
- 快速响应 vs 执行器寿命

**试探法**：
1. 先设 $R = I$，调整 $Q$ 使性能合理
2. 固定 $Q$，调整 $R$ 改变控制平滑度
3. 反复迭代

#### 终端代价 P

**作用**：保证稳定性，改善长期性能

**设计方法**：

**方法1：离散时间Riccati方程**

$$P = A^T P A - A^T P B(R + B^T P B)^{-1}B^T P A + Q$$

**方法2：连续时间转换**

先求连续时间LQR，再离散化。

**方法3：简化设置**

$$P = \alpha Q, \quad \alpha > 1$$

例如 $\alpha = 10$。

### 5.1.7 约束处理技术

#### 硬约束 vs 软约束

**硬约束**（Hard Constraints）：
- 必须严格满足
- 可能导致优化不可行

**软约束**（Soft Constraints）：
- 允许轻微违反
- 引入松弛变量（slack variable）

**软约束实现**：

$$\min J + \rho \|\epsilon\|^2$$

$$\text{s.t.} \quad g(x, u) \leq \epsilon, \quad \epsilon \geq 0$$

其中 $\rho$ 是惩罚权重（很大）。

#### 不可行性处理

**问题**：当前状态下，所有可行控制都无法满足约束。

**解决方案**：

1. **放松状态约束**：
   - 输入约束（硬）保持
   - 状态约束变为软约束

2. **可行性MPC**：
   - 设计保证始终可行
   - 使用不变集理论

3. **应急模式**：
   - 切换到备用控制器（如PID）
   - 进入安全状态

### 5.1.8 MPC实现要点

#### 采样时间选择

**准则**：

$$\Delta t \leq \frac{T_{settling}}{10 \sim 20}$$

其中 $T_{settling}$ 是系统调节时间。

**考虑因素**：
- 系统动态快慢
- 计算时间限制
- 传感器采样频率
- 执行器响应速度

#### 状态估计

**问题**：通常无法测量所有状态。

**解决方案**：

**卡尔曼滤波器**（线性系统）：

$$\hat{x}_{k+1|k} = A\hat{x}_k + Bu_k$$
$$\hat{x}_{k+1} = \hat{x}_{k+1|k} + L(y_{k+1} - C\hat{x}_{k+1|k})$$

**扩展卡尔曼滤波（EKF）**（非线性系统）：

线性化 + 卡尔曼滤波

**移动窗口估计（MHE）**：

类似MPC的思想，优化估计状态。

#### 模型不匹配

**挑战**：实际系统与模型有偏差

**鲁棒性增强**：

1. **增大终端权重**：
   - $P$ 较大 → 对模型误差不太敏感

2. **较短预测时域**：
   - $N$ 不要太大 → 减少累积误差

3. **积分作用**：
   - 引入积分状态消除稳态偏差
   $$x_{int,k+1} = x_{int,k} + (r - y_k)\Delta t$$

4. **自适应MPC**：
   - 在线辨识模型参数
   - 实时更新 $A, B$

### 5.1.9 本项目中的MPC实现

#### 温度控制系统

**文件**：`MPCController/mpc_temperature_control.py`

**系统模型**：

$$\frac{dT}{dt} = -a(T - T_{amb}) + bu$$

**离散化**：

$$T_{k+1} = T_k + [-a(T_k - T_{amb}) + bu_k]\Delta t$$

**MPC参数**：
```python
N = 20              # 预测200秒
M = 10              # 控制100秒
dt = 10.0           # 采样10秒
Q_weight = 10.0     # 温度误差权重
R_weight = 0.001    # 控制权重
u_min, u_max = 0, 2000  # 功率约束
```

**运行实验**：
```bash
python start.py mpc-temp
```

**关键代码**（第234-265行）：
```python
def update(self, T_current):
    # 初始猜测
    u0 = np.zeros(self.M)
    
    # 约束
    bounds = [(self.u_min, self.u_max) for _ in range(self.M)]
    
    # 优化
    result = minimize(
        fun=lambda u: self.cost_function(u, T_current),
        x0=u0,
        method='SLSQP',
        bounds=bounds
    )
    
    return result.x[0]  # 返回第一个控制
```

#### CartPole系统

**文件**：`MPCController/mpc_controller.py`

**系统模型**：

$$x_{k+1} = f(x_k, u_k) = \begin{bmatrix} 
x + \dot{x}\Delta t \\
\dot{x} + \ddot{x}\Delta t \\
\theta + \dot{\theta}\Delta t \\
\dot{\theta} + \ddot{\theta}\Delta t
\end{bmatrix}$$

其中 $\ddot{x}, \ddot{\theta}$ 由非线性动力学计算。

**MPC参数**：
```python
N = 10 or 15        # 预测步数
M = 5 or 8          # 控制步数
dt = 0.02           # 20ms
Q = diag([10, 1, 200, 20])
R = 0.01
u_min, u_max = -50, 50  # 力约束
```

**运行实验**：
```bash
python start.py mpc
```

**结果对比**（见 `output/mpc_cartpole_comparison.png`）：
- PID：0.10s（失败）
- 非线性MPC：3.15s
- 线性MPC：4.09s

---

## 5.2 优化与约束控制理论

### 5.2.1 优化控制基础

#### 最优控制问题

**Bolza形式**：

$$\min_{u(t)} J = \phi(x(T)) + \int_0^T L(x(t), u(t), t)dt$$

$$\text{s.t.} \quad \dot{x} = f(x, u, t), \quad x(0) = x_0$$

其中：
- $\phi(x(T))$：终端代价（Mayer项）
- $L(x, u, t)$：阶段代价（Lagrange项）

#### 变分法与极小值原理

**必要条件（Pontryagin极小值原理）**：

定义Hamiltonian：

$$H(x, u, \lambda, t) = L(x, u, t) + \lambda^T f(x, u, t)$$

最优解满足：
1. **状态方程**：$\dot{x} = \frac{\partial H}{\partial \lambda}$
2. **协态方程**：$\dot{\lambda} = -\frac{\partial H}{\partial x}$
3. **最优性条件**：$\frac{\partial H}{\partial u} = 0$（或 $u$ 最小化 $H$）
4. **边界条件**：$x(0) = x_0$, $\lambda(T) = \frac{\partial \phi}{\partial x}|_{x(T)}$

#### 线性二次型调节器（LQR）回顾

**无限时域LQR**：

$$\min \int_0^\infty (x^T Q x + u^T R u)dt$$

$$\text{s.t.} \quad \dot{x} = Ax + Bu$$

**解**：

$$u^* = -Kx, \quad K = R^{-1}B^T P$$

其中 $P$ 满足代数Riccati方程（ARE）：

$$A^T P + PA - PBR^{-1}B^T P + Q = 0$$

**有限时域LQR**：

微分Riccati方程（DRE）：

$$-\dot{P} = A^T P + PA - PBR^{-1}B^T P + Q$$

边界条件：$P(T) = P_f$

### 5.2.2 约束优化方法

#### 拉格朗日乘子法

**等式约束优化**：

$$\min f(x) \quad \text{s.t.} \quad h(x) = 0$$

**Lagrangian**：

$$\mathcal{L}(x, \lambda) = f(x) + \lambda^T h(x)$$

**KKT条件**：

$$\nabla_x \mathcal{L} = 0, \quad h(x) = 0$$

#### KKT条件（不等式约束）

**优化问题**：

$$\min f(x) \quad \text{s.t.} \quad g(x) \leq 0, \quad h(x) = 0$$

**KKT条件**：

1. **平稳性**：$\nabla f(x) + \mu^T \nabla g(x) + \lambda^T \nabla h(x) = 0$
2. **原始可行性**：$g(x) \leq 0$, $h(x) = 0$
3. **对偶可行性**：$\mu \geq 0$
4. **互补松弛性**：$\mu_i g_i(x) = 0$

#### 障碍函数法（Interior Point）

**思想**：用barrier函数近似不等式约束

$$g(x) \leq 0 \quad \Rightarrow \quad -\frac{1}{t}\log(-g(x))$$

**barrier问题**：

$$\min f(x) - \frac{1}{t}\sum_i \log(-g_i(x))$$

逐步增大 $t$，逼近原问题。

#### 罚函数法（Penalty Method）

**外罚函数**：

$$\min f(x) + c \sum_i \max(0, g_i(x))^2$$

逐步增大 $c$。

### 5.2.3 数值优化算法

#### 梯度下降法

$$x_{k+1} = x_k - \alpha_k \nabla f(x_k)$$

**问题**：收敛慢，对非凸函数可能不收敛

#### 牛顿法

$$x_{k+1} = x_k - [\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$$

**优点**：二次收敛
**缺点**：需要Hessian矩阵，计算量大

#### 拟牛顿法（BFGS）

用矩阵近似Hessian，避免直接计算。

#### 共轭梯度法

改进的梯度方向，加速收敛。

#### 序列二次规划（SQP）

迭代求解QP子问题逼近原问题。

---

## 5.3 自适应控制

### 5.3.1 基本概念

**定义**：控制器参数或结构根据系统特性变化自动调整。

**适用场景**：
- 参数缓慢时变（磨损、老化）
- 工况变化（负载、环境）
- 参数未知但可观测

**分类**：
1. **模型参考自适应控制（MRAC）**
2. **自校正调节器（STR）**
3. **增益调度（Gain Scheduling）**

### 5.3.2 模型参考自适应控制（MRAC）

**思想**：设计参考模型，使实际系统输出跟踪参考模型输出。

**结构**：

```
       参考输入 r
          │
          ↓
    ┌──────────┐      y_m
    │ 参考模型 │─────→ (理想输出)
    └──────────┘
                         ↑
                    误差 e = y - y_m
                         │
    ┌──────────┐      y  │
    │ 实际系统 │─────────┤
    └──────────┘         │
          ↑              │
          │              ↓
    ┌─────────────────────┐
    │  自适应律(调参)      │
    └─────────────────────┘
```

**自适应律**（MIT规则）：

$$\frac{d\theta}{dt} = -\gamma e \frac{\partial e}{\partial \theta}$$

其中 $\theta$ 是待调参数，$\gamma$ 是学习率。

**稳定性**：需要Lyapunov理论分析

### 5.3.3 自校正调节器（STR）

**思想**：在线辨识系统模型，基于辨识模型设计控制器。

**步骤**：
1. **系统辨识**：估计模型参数
   - 最小二乘法（RLS）
   - 递推最小二乘（RLS）
   
2. **控制器设计**：根据估计模型设计
   - 极点配置
   - 最小方差控制
   
3. **控制执行**：应用设计的控制器

**确定性等价原则**：
把参数估计值当作真值使用（虽然有误差）。

### 5.3.4 增益调度（Gain Scheduling）

**思想**：根据工作点切换控制器参数。

**步骤**：
1. 选择调度变量（如速度、高度）
2. 在多个工作点设计控制器
3. 在线插值获得当前参数

**示例**：飞机控制
- 低速：高升力，小阻尼
- 高速：低升力，大阻尼
- 根据飞行速度调度控制增益

**优点**：
- ✅ 设计简单（多个线性控制器）
- ✅ 计算量小

**缺点**：
- ❌ 需要准确调度变量
- ❌ 切换可能不平滑

### 5.3.5 自适应MPC

**思想**：在线辨识模型，实时更新MPC预测模型。

**步骤**：
1. 收集输入输出数据
2. 递推辨识系统参数（RLS、Kalman滤波）
3. 更新MPC中的 $A, B$ 矩阵
4. 求解更新后的MPC问题

**挑战**：
- 辨识与控制的"双重最优化"冲突
- 探索（激励信号）vs 利用（性能）权衡
- 稳定性保证困难

---

## 5.4 鲁棒控制

### 5.4.1 不确定性建模

**参数不确定性**：

$$\dot{x} = (A + \Delta A)x + (B + \Delta B)u$$

其中 $\Delta A, \Delta B$ 是未知但有界的摄动。

**建模方式**：

1. **范数有界不确定性**：
   $$\|\Delta A\| \leq \epsilon$$

2. **结构化不确定性**：
   $$\Delta A = \sum_i \delta_i E_i, \quad |\delta_i| \leq 1$$

3. **多胞不确定性**：
   $$(A, B) \in \text{Conv}\{(A_1, B_1), \ldots, (A_N, B_N)\}$$

### 5.4.2 H∞控制

**目标**：最小化系统对扰动的最大增益。

**H∞范数**：

$$\|G\|_\infty = \sup_{\omega} \sigma_{max}(G(j\omega))$$

其中 $\sigma_{max}$ 是最大奇异值。

**H∞优化问题**：

$$\min_{K} \|T_{zw}\|_\infty$$

其中 $T_{zw}$ 是从扰动 $w$ 到输出 $z$ 的传递函数。

**求解**：Riccati方程

### 5.4.3 滑模控制（Sliding Mode Control）

**思想**：设计滑模面，使系统状态快速到达并保持在滑模面上。

**滑模面**：

$$s(x) = Cx = 0$$

**控制律**：

$$u = u_{eq} + u_{sw}$$

其中：
- $u_{eq}$：等效控制（保持在滑模面）
- $u_{sw}$：切换控制（驱动到滑模面）

**切换律**：

$$u_{sw} = -k \cdot \text{sign}(s)$$

**优点**：
- ✅ 对参数变化不敏感
- ✅ 对匹配扰动完全鲁棒

**缺点**：
- ❌ 抖振（chattering）问题
- ❌ 执行器磨损

**抖振抑制**：
- 边界层（boundary layer）
- 高阶滑模

### 5.4.4 μ综合

**处理结构化不确定性**的系统化方法。

**结构化奇异值**：

$$\mu(\Delta) = \frac{1}{\min\{\bar{\sigma}(\Delta) : \det(I - M\Delta) = 0\}}$$

**鲁棒性能条件**：

$$\mu(M) < 1$$

**设计**：D-K迭代

---

## 5.5 前馈控制

### 5.5.1 前馈控制原理

**思想**：对可测扰动或已知参考变化进行预补偿。

**结构**：

```
     r(t)
      │
      ├──→ [前馈控制器 Gf] ──→ ⊕ ──→ [系统 G] ──→ y(t)
      │                        ↑          │
      └──→ [反馈控制器 Gc] ←───┴──────────┘
```

**目标**：$G(s) \cdot G_f(s) = 1$（理想）

实际：$G_f(s) = G^{-1}(s)$（逆系统）

### 5.5.2 前馈设计方法

#### 完全补偿

**条件**：模型精确已知，且 $G(s)$ 可逆

**前馈控制器**：

$$G_f(s) = \frac{1}{G(s)}$$

**问题**：
- 物理不可实现（非因果）
- 模型误差敏感

#### 近似补偿

**低通滤波**：

$$G_f(s) = \frac{1}{G(s)} \cdot \frac{1}{\tau s + 1}$$

使系统因果可实现。

### 5.5.3 前馈+反馈组合

**优势互补**：
- 前馈：快速、主动
- 反馈：鲁棒、纠错

**应用场景**：
- CNC机床：前馈补偿轨迹，反馈纠正误差
- 自适应巡航：前馈预测车辆行为，反馈保持车距

---

## 5.6 人工智能控制（强化学习）

### 5.6.1 强化学习基础

**核心概念**：

```
    环境（Environment）
         ↓ 状态 s_t
    智能体（Agent）
         ↓ 动作 a_t
    环境
         ↓ 奖励 r_t, 新状态 s_{t+1}
    （循环）
```

**马尔可夫决策过程（MDP）**：

- **状态空间**：$\mathcal{S}$
- **动作空间**：$\mathcal{A}$
- **转移概率**：$P(s'|s, a)$
- **奖励函数**：$R(s, a)$

**目标**：找策略 $\pi(a|s)$ 最大化累积回报

$$J(\pi) = \mathbb{E}\left[\sum_{t=0}^\infty \gamma^t r_t\right]$$

其中 $\gamma \in [0, 1)$ 是折扣因子。

### 5.6.2 主流RL算法

#### 值函数方法

**Q-learning**：

$$Q(s, a) \leftarrow Q(s, a) + \alpha[r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$$

**Deep Q-Network (DQN)**：
- 用神经网络逼近Q函数
- 经验回放（Experience Replay）
- 目标网络（Target Network）

#### 策略梯度方法

**REINFORCE**：

$$\nabla_\theta J(\theta) = \mathbb{E}_\pi[G_t \nabla_\theta \log \pi_\theta(a_t|s_t)]$$

**Actor-Critic**：
- Actor：策略网络
- Critic：价值网络
- 减少方差

**PPO（Proximal Policy Optimization）**：
- 限制策略更新幅度
- 稳定训练

**SAC（Soft Actor-Critic）**：
- 最大熵RL
- 鼓励探索

### 5.6.3 RL在控制中的应用

#### CartPole示例

**状态**：$[x, \dot{x}, \theta, \dot{\theta}]$
**动作**：左移/右移力
**奖励**：保持竖直+1，倒下-100

**训练**：
- DQN：1000-5000 episodes
- PPO：500-2000 episodes

**结果**：可无限保持平衡

#### RL vs MPC vs PID

| 对比项 | PID | MPC | RL |
|--------|-----|-----|-----|
| **模型需求** | 无 | 需要准确模型 | 无 |
| **训练需求** | 无 | 无 | 大量训练 |
| **性能** | 限于线性 | 优化最优 | 可达极致 |
| **约束** | 简单截断 | 显式处理 | 难保证 |
| **可解释性** | 高 | 高 | 低（黑盒） |
| **实时性** | 极快 | 依赖求解器 | 快（推理） |
| **泛化** | 限于相似系统 | 限于模型准确 | 可泛化 |

### 5.6.4 RL与MPC结合

**混合架构**：

1. **MPC为安全层**：
   - RL学习策略
   - MPC保证约束

2. **RL学习模型**：
   - RL学习系统动力学
   - MPC基于学习模型优化

3. **MPC指导RL**：
   - MPC生成示范轨迹
   - RL模仿并超越

---

## 5.7 方法对比与选型指南

### 5.7.1 性能对比矩阵

| 方法 | 多变量 | 约束 | 最优性 | 鲁棒性 | 计算量 | 实现难度 |
|------|--------|------|--------|--------|--------|---------|
| **PID** | ⭐ | ⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| **LQR/LQG** | ⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ |
| **MPC** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐ |
| **自适应** | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ |
| **鲁棒(H∞)** | ⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐ |
| **滑模** | ⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐ |
| **RL** | ⭐⭐⭐⭐⭐ | ⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | 训练⭐/推理⭐⭐⭐⭐ | ⭐ |

### 5.7.2 选型决策树

```
需要控制系统？
    │
    ├─ 系统简单（1-2阶，线性）？
    │   └─ YES → PID（加前馈/串级）
    │
    ├─ 有准确数学模型？
    │   ├─ YES → 有约束？
    │   │         ├─ YES → MPC
    │   │         └─ NO → LQR/LQG
    │   └─ NO → 继续
    │
    ├─ 参数/工况变化大？
    │   └─ YES → 自适应控制或增益调度
    │
    ├─ 不确定性严重？
    │   └─ YES → 鲁棒控制（H∞/滑模）
    │
    ├─ 可测扰动？
    │   └─ YES → 前馈+反馈
    │
    └─ 极致性能+可大量训练？
        └─ YES → 强化学习（或RL+MPC）
```

### 5.7.3 工程实践建议

**渐进式设计策略**：

1. **第一步**：从PID开始
   - 快速验证可控性
   - 建立性能基准

2. **第二步**：根据问题引入先进方法
   - 多变量耦合 → MPC
   - 参数变化 → 自适应
   - 不确定性 → 鲁棒

3. **第三步**：组合方法
   - 前馈+PID
   - MPC+状态估计
   - RL+MPC混合

**调试与验证**：
- 仿真先行（MATLAB/Python）
- 逐步复杂化（先线性后非线性）
- 实时性评估（计算时间测试）
- 鲁棒性测试（参数摄动、扰动）

---

## 5.8 本章小结

### 核心知识点回顾

#### MPC核心

$$\min_{u_0, \ldots, u_{M-1}} \sum_{i=0}^{N-1} \|x_i - x_{ref}\|_Q^2 + \sum_{i=0}^{M-1} \|u_i\|_R^2$$

$$\text{s.t.} \quad x_{k+1} = f(x_k, u_k), \quad u_{min} \leq u_k \leq u_{max}$$

**四大要素**：预测、优化、约束、滚动

#### 各方法特点

| 方法 | 核心思想 | 关键优势 | 主要挑战 |
|------|---------|---------|---------|
| **MPC** | 预测+优化 | 约束处理、最优 | 计算量、模型依赖 |
| **自适应** | 在线调参 | 适应时变 | 稳定性保证 |
| **鲁棒** | 最坏保证 | 对不确定鲁棒 | 保守性 |
| **前馈** | 预补偿 | 快速响应 | 模型敏感 |
| **RL** | 学习策略 | 无需模型、高性能 | 训练代价、安全性 |

### 本项目中的实践

| 理论内容 | 项目实现 | 文件/命令 |
|---------|---------|----------|
| **MPC原理** | 温度控制 | `python start.py mpc-temp` |
| **非线性MPC** | CartPole | `python start.py mpc` |
| **MPC实现** | 完整代码 | `MPCController/mpc_controller.py` |
| **性能对比** | PID vs MPC | `output/mpc_temperature_control.png` |
| **详细说明** | MPC文档 | `doc/MPC控制器说明.md` |
| **RL对比** | PID vs RL | `doc/PID_vs_RL_对比.md` |

### 学习建议

**理论学习路径**：
1. MPC基础（本章5.1）
2. 运行温度控制实验（直观理解）
3. 优化理论（本章5.2）
4. CartPole实验（体验挑战）
5. 其他方法概览（5.3-5.6）

**动手实践**：
```bash
# Step 1: 温度MPC（入门）
python start.py mpc-temp

# Step 2: CartPole MPC（进阶）
python start.py mpc

# Step 3: 修改参数实验
# 编辑 MPCController/mpc_controller.py
# 调整 N, M, Q, R 观察效果
```

**深入方向**：
- 学习凸优化（Boyd教材）
- 实现显式MPC
- 尝试自适应MPC
- 探索RL算法（Stable-Baselines3）

---

## 📚 扩展阅读

### 经典教材

**MPC**：
1. **"Model Predictive Control"** - Camacho & Bordons
   - 工业应用导向
   - 实例丰富

2. **"Predictive Control"** - Maciejowski
   - 理论严谨
   - 多变量系统

3. **"Constrained MPC"** - Mayne et al.
   - 稳定性理论
   - 顶级综述

**优化**：
4. **"Convex Optimization"** - Boyd & Vandenberghe
   - 凸优化圣经
   - 免费在线

**自适应/鲁棒**：
5. **"Adaptive Control"** - Åström & Wittenmark
6. **"Robust Control"** - Zhou & Doyle

**强化学习**：
7. **"Reinforcement Learning"** - Sutton & Barto
   - RL圣经
   - 第二版免费

### 软件工具

**MPC工具**：
- **CasADi**：符号框架，Python/MATLAB
- **do-mpc**：Python MPC库
- **ACADO**：快速NMPC
- **FORCES Pro**：商业，超快

**优化求解器**：
- **OSQP**：开源QP
- **qpOASES**：实时QP
- **IPOPT**：大规模NLP
- **CPLEX/Gurobi**：商业，最快

**RL工具**：
- **Stable-Baselines3**：易用的RL库
- **RLlib**（Ray）：分布式RL
- **OpenAI Gym**：标准环境

### 在线资源

- MIT OCW 16.323 - Principles of Optimal Control
- Udacity - Self-Driving Car Nanodegree（MPC项目）
- Steve Brunton - Control Bootcamp（YouTube）
- Brian Douglas - Control Systems Lectures

---

## 附录：关键公式速查

### MPC优化问题

$$\min \sum_{i=1}^{N} \|x_i - x_{ref}\|_Q^2 + \sum_{i=0}^{M-1} \|u_i\|_R^2$$

$$\text{s.t.} \quad x_{i+1} = Ax_i + Bu_i, \quad u_{min} \leq u_i \leq u_{max}$$

### LQR解

$$u = -Kx, \quad K = R^{-1}B^T P$$

$$A^T P + PA - PBR^{-1}B^T P + Q = 0$$

### Pontryagin极小值原理

$$H = L(x, u) + \lambda^T f(x, u)$$

$$\dot{x} = \frac{\partial H}{\partial \lambda}, \quad \dot{\lambda} = -\frac{\partial H}{\partial x}, \quad \frac{\partial H}{\partial u} = 0$$

---

> **导航**: [返回课程大纲](../CONTROL_ENGINEERING_OUTLINE.md) | [上一章：现代控制理论](第4章_现代控制理论基础.md) | [下一章：典型工程案例](第6章_典型工程案例分析.md)

---

**最后更新**: 2025年10月30日  
**作者**: Control Engineering Lab
